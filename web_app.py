from flask import Flask, render_template, request, jsonify, redirect, url_for, send_file, make_response
import os
import json
import glob
from datetime import datetime
from drone_communication_analyzer import DroneCommAnalyzer
from visualization import DroneCommVisualizer, create_summary_dashboard
import plotly
import plotly.utils
import io
import zipfile
import tempfile
import pandas as pd
import shutil
from werkzeug.utils import secure_filename
import math

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆç”¨äºç”Ÿäº§éƒ¨ç½²ï¼‰
os.environ.setdefault('FLASK_ENV', 'production')

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024  # 500MB max file size
app.config['UPLOAD_FOLDER'] = 'uploads'

# ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# å…¨å±€å˜é‡å­˜å‚¨å½“å‰åˆ†æå™¨å’Œå¯è§†åŒ–å™¨
current_analyzer = None
current_visualizer = None
available_datasets = []

# æ·»åŠ ç¼“å­˜æ§åˆ¶è£…é¥°å™¨
def add_cache_control(response):
    """æ·»åŠ ç¼“å­˜æ§åˆ¶å¤´"""
    response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate, max-age=0'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Expires'] = '0'
    response.headers['Last-Modified'] = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    response.headers['ETag'] = str(hash(datetime.now().isoformat()))
    return response

@app.after_request
def after_request(response):
    """ä¸ºæ‰€æœ‰å“åº”æ·»åŠ ç¼“å­˜æ§åˆ¶"""
    return add_cache_control(response)

def scan_available_datasets():
    """æ‰«æå¯ç”¨çš„æ•°æ®é›†"""
    global available_datasets
    available_datasets = []
    
    # é¦–å…ˆæ£€æŸ¥æ ¹ç›®å½•ä¸‹çš„æ•°æ®é›†æ–‡ä»¶å¤¹
    for item in os.listdir('.'):
        item_path = os.path.join('.', item)
        if os.path.isdir(item_path) and item not in ['data', 'uploads', 'templates', 'static', '__pycache__', '.git']:
            sender_path = os.path.join(item_path, 'sender')
            receiver_path = os.path.join(item_path, 'receiver')
            if os.path.exists(sender_path) and os.path.exists(receiver_path):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ•°æ®æ–‡ä»¶
                has_udp = (bool(glob.glob(os.path.join(sender_path, 'udp_sender_*.csv'))) and 
                          bool(glob.glob(os.path.join(receiver_path, 'udp_receiver_*.csv'))))
                has_nexfi = (bool(glob.glob(os.path.join(sender_path, 'nexfi_status_*.csv'))) or
                           bool(glob.glob(os.path.join(receiver_path, 'nexfi_status_*.csv'))))
                has_gps = (bool(glob.glob(os.path.join(sender_path, 'gps_logger_*.csv'))) or
                          bool(glob.glob(os.path.join(receiver_path, 'gps_logger_*.csv'))))
                
                dataset_info = {
                    'name': item,
                    'path': item_path,
                    'has_udp': has_udp,
                    'has_nexfi': has_nexfi,
                    'has_gps': has_gps,
                    'creation_time': datetime.fromtimestamp(os.path.getctime(item_path)).strftime('%Y-%m-%d %H:%M:%S'),
                    'source': 'local'
                }
                available_datasets.append(dataset_info)
    
    # ç„¶åæ£€æŸ¥dataç›®å½•ä¸‹çš„æ•°æ®é›†
    data_dir = 'data'
    if os.path.exists(data_dir):
        for item in os.listdir(data_dir):
            item_path = os.path.join(data_dir, item)
            if os.path.isdir(item_path):
                sender_path = os.path.join(item_path, 'sender')
                receiver_path = os.path.join(item_path, 'receiver')
                if os.path.exists(sender_path) and os.path.exists(receiver_path):
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ•°æ®æ–‡ä»¶
                    has_udp = (bool(glob.glob(os.path.join(sender_path, 'udp_sender_*.csv'))) and 
                              bool(glob.glob(os.path.join(receiver_path, 'udp_receiver_*.csv'))))
                    has_nexfi = (bool(glob.glob(os.path.join(sender_path, 'nexfi_status_*.csv'))) or
                               bool(glob.glob(os.path.join(receiver_path, 'nexfi_status_*.csv'))))
                    has_gps = (bool(glob.glob(os.path.join(sender_path, 'gps_logger_*.csv'))) or
                              bool(glob.glob(os.path.join(receiver_path, 'gps_logger_*.csv'))))
                    
                    dataset_info = {
                        'name': item,
                        'path': item_path,
                        'has_udp': has_udp,
                        'has_nexfi': has_nexfi,
                        'has_gps': has_gps,
                        'creation_time': datetime.fromtimestamp(os.path.getctime(item_path)).strftime('%Y-%m-%d %H:%M:%S'),
                        'source': 'data'
                    }
                    available_datasets.append(dataset_info)
    
    # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
    available_datasets.sort(key=lambda x: x['creation_time'], reverse=True)


def validate_dataset_structure(dataset_path):
    """éªŒè¯æ•°æ®é›†ç»“æ„æ˜¯å¦æ­£ç¡®"""
    sender_path = os.path.join(dataset_path, 'sender')
    receiver_path = os.path.join(dataset_path, 'receiver')
    
    if not os.path.exists(sender_path) or not os.path.exists(receiver_path):
        return False, "ç¼ºå°‘senderæˆ–receiveræ–‡ä»¶å¤¹"
    
    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶
    required_files = {
        'udp_sender': glob.glob(os.path.join(sender_path, 'udp_sender_*.csv')),
        'udp_receiver': glob.glob(os.path.join(receiver_path, 'udp_receiver_*.csv')),
    }
    
    missing_files = []
    for file_type, files in required_files.items():
        if not files:
            missing_files.append(file_type)
    
    if missing_files:
        return False, f"ç¼ºå°‘å¿…è¦æ–‡ä»¶: {', '.join(missing_files)}"
    
    return True, "æ•°æ®é›†ç»“æ„æ­£ç¡®"


def extract_zip_dataset(zip_path, extract_to):
    """è§£å‹ZIPæ–‡ä»¶å¹¶éªŒè¯æ•°æ®é›†ç»“æ„"""
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            # è·å–ZIPæ–‡ä»¶ä¸­çš„æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨
            file_list = zip_ref.namelist()
            
            # æŸ¥æ‰¾æ•°æ®é›†æ ¹ç›®å½•
            dataset_folders = set()
            for file_path in file_list:
                parts = file_path.split('/')
                if len(parts) >= 3:  # è‡³å°‘åŒ…å« dataset/sender|receiver/file
                    if parts[1] in ['sender', 'receiver']:
                        dataset_folders.add(parts[0])
            
            if not dataset_folders:
                return None, "ZIPæ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®é›†ç»“æ„"
            
            if len(dataset_folders) > 1:
                return None, "ZIPæ–‡ä»¶ä¸­åŒ…å«å¤šä¸ªæ•°æ®é›†ï¼Œè¯·ç¡®ä¿åªåŒ…å«ä¸€ä¸ªæ•°æ®é›†"
            
            dataset_name = list(dataset_folders)[0]
            
            # è§£å‹åˆ°ä¸´æ—¶ç›®å½•
            temp_extract_path = os.path.join(extract_to, 'temp_extract')
            zip_ref.extractall(temp_extract_path)
            
            # ç§»åŠ¨æ•°æ®é›†åˆ°æ­£ç¡®ä½ç½®
            source_dataset_path = os.path.join(temp_extract_path, dataset_name)
            target_dataset_path = os.path.join(extract_to, dataset_name)
            
            if os.path.exists(target_dataset_path):
                # å¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                target_dataset_path = os.path.join(extract_to, f"{dataset_name}_{timestamp}")
            
            shutil.move(source_dataset_path, target_dataset_path)
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            shutil.rmtree(temp_extract_path)
            
            # éªŒè¯æ•°æ®é›†ç»“æ„
            is_valid, message = validate_dataset_structure(target_dataset_path)
            if not is_valid:
                shutil.rmtree(target_dataset_path)
                return None, f"æ•°æ®é›†ç»“æ„éªŒè¯å¤±è´¥: {message}"
            
            return os.path.basename(target_dataset_path), "æ•°æ®é›†ä¸Šä¼ æˆåŠŸ"
            
    except zipfile.BadZipFile:
        return None, "æ— æ•ˆçš„ZIPæ–‡ä»¶"
    except Exception as e:
        return None, f"è§£å‹å¤±è´¥: {str(e)}"


@app.route('/')
def index():
    """ä¸»é¡µé¢ï¼Œæ˜¾ç¤ºå¯ç”¨æ•°æ®é›†åˆ—è¡¨"""
    scan_available_datasets()
    return render_template('index.html', datasets=available_datasets)


@app.route('/upload', methods=['POST'])
def upload_dataset():
    """ä¸Šä¼ æ•°æ®é›†ZIPæ–‡ä»¶"""
    if 'dataset_file' not in request.files:
        return jsonify({'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400
    
    file = request.files['dataset_file']
    if file.filename == '':
        return jsonify({'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400
    
    if not file.filename.lower().endswith('.zip'):
        return jsonify({'error': 'åªæ”¯æŒZIPæ ¼å¼æ–‡ä»¶'}), 400
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        filename = secure_filename(file.filename)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_filename = f"{timestamp}_{filename}"
        zip_path = os.path.join(app.config['UPLOAD_FOLDER'], safe_filename)
        file.save(zip_path)
        
        # è§£å‹å¹¶éªŒè¯æ•°æ®é›†
        dataset_name, message = extract_zip_dataset(zip_path, 'data')
        
        # åˆ é™¤ä¸Šä¼ çš„ZIPæ–‡ä»¶
        os.remove(zip_path)
        
        if dataset_name is None:
            return jsonify({'error': message}), 400
        
        # é‡æ–°æ‰«ææ•°æ®é›†
        scan_available_datasets()
        
        return jsonify({
            'success': True,
            'message': message,
            'dataset_name': dataset_name
        })
        
    except Exception as e:
        return jsonify({'error': f'ä¸Šä¼ å¤±è´¥: {str(e)}'}), 500


@app.route('/analyze/<dataset_name>')
def analyze_dataset(dataset_name):
    """åˆ†ææŒ‡å®šæ•°æ®é›†"""
    global current_analyzer, current_visualizer
    
    # å¼ºåˆ¶æ¸…ç†æ—§çš„åˆ†æç»“æœ
    current_analyzer = None
    current_visualizer = None
    
    # æŸ¥æ‰¾æ•°æ®é›†è·¯å¾„
    dataset_path = None
    for dataset in available_datasets:
        if dataset['name'] == dataset_name:
            dataset_path = dataset['path']
            break
    
    if not dataset_path or not os.path.exists(dataset_path):
        return jsonify({'error': f'æ•°æ®é›† {dataset_name} ä¸å­˜åœ¨'}), 404
    
    try:
        print(f"å¼€å§‹åˆ†ææ•°æ®é›†: {dataset_name}")
        print(f"æ•°æ®é›†è·¯å¾„: {dataset_path}")
        
        # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œåˆ†æ
        current_analyzer = DroneCommAnalyzer(dataset_path)
        current_analyzer.run_full_analysis()
        
        print("æ•°æ®åˆ†æå®Œæˆï¼Œå¼€å§‹ç”Ÿæˆå¯è§†åŒ–...")
        
        # åˆ›å»ºå¯è§†åŒ–å™¨å¹¶ç”Ÿæˆå›¾è¡¨
        current_visualizer = DroneCommVisualizer(current_analyzer)
        current_visualizer.create_all_plots()
        
        print("å¯è§†åŒ–ç”Ÿæˆå®Œæˆ")
        
        # åˆ›å»ºå“åº”å¹¶æ·»åŠ ç¼“å­˜æ§åˆ¶
        response = make_response(redirect(url_for('dashboard', dataset_name=dataset_name)))
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
        
        return response
        
    except Exception as e:
        print(f"åˆ†æé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'åˆ†æå¤±è´¥: {str(e)}'}), 500


@app.route('/dashboard/<dataset_name>')
def dashboard(dataset_name):
    """æ˜¾ç¤ºåˆ†æä»ªè¡¨æ¿"""
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ†æ
    force_reanalyze = request.args.get('force', 'false').lower() == 'true'
    
    if current_analyzer is None or force_reanalyze:
        print(f"éœ€è¦é‡æ–°åˆ†ææ•°æ®é›†: {dataset_name}")
        return redirect(url_for('analyze_dataset', dataset_name=dataset_name))
    
    # åˆ›å»ºå“åº”å¹¶æ·»åŠ ç¼“å­˜æ§åˆ¶
    response = make_response(render_template('dashboard.html', dataset_name=dataset_name))
    response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Expires'] = '0'
    
    return response


@app.route('/api/figures')
def get_figures():
    """APIç«¯ç‚¹ï¼šè·å–æ‰€æœ‰å›¾è¡¨çš„JSONæ•°æ®"""
    if current_visualizer is None:
        return jsonify({'error': 'å°šæœªè¿›è¡Œåˆ†æ'}), 400
    
    try:
        figures_json = {}
        for name, fig in current_visualizer.figures.items():
            figures_json[name] = json.loads(fig.to_json())
        
        print(f"è¿”å›å›¾è¡¨æ•°æ®ï¼ŒåŒ…å« {len(figures_json)} ä¸ªå›¾è¡¨")
        return jsonify(figures_json)
        
    except Exception as e:
        print(f"è·å–å›¾è¡¨æ•°æ®é”™è¯¯: {str(e)}")
        return jsonify({'error': f'è·å–å›¾è¡¨æ•°æ®å¤±è´¥: {str(e)}'}), 500


@app.route('/api/summary')
def api_summary():
    """è¿”å›åˆ†æç»“æœæ‘˜è¦"""
    if not current_analyzer or not current_analyzer.analysis_results:
        return jsonify({'error': 'No analysis results available'}), 404
    
    summary = {
        'udp': None,
        'distance': None,
        'gps': {},
        'nexfi': {},
        'correlations': None,
        'time_range': None  # æ·»åŠ æ—¶é—´èŒƒå›´ä¿¡æ¯
    }
    
    # UDPç»Ÿè®¡
    if 'udp' in current_analyzer.analysis_results:
        udp_stats = current_analyzer.analysis_results['udp']
        summary['udp'] = {
            'total_sent': udp_stats['total_sent'],
            'total_received': udp_stats['total_received'],
            'packet_loss_rate': f"{udp_stats['packet_loss_rate']:.2f}",
            'avg_delay': f"{udp_stats['delay_stats']['mean']:.2f}",
            'max_delay': f"{udp_stats['delay_stats']['max']:.2f}",
            'throughput': f"{udp_stats['throughput_kbps']:.2f}",
            'test_duration': f"{udp_stats['test_duration']:.1f}"
        }
    
    # è·ç¦»ç»Ÿè®¡
    if 'inter_drone_distance' in current_analyzer.analysis_results:
        dist_stats = current_analyzer.analysis_results['inter_drone_distance']
        summary['distance'] = {
            'min_distance_3d': f"{dist_stats['min_distance_3d']:.2f}",
            'max_distance_3d': f"{dist_stats['max_distance_3d']:.2f}",
            'mean_distance_3d': f"{dist_stats['mean_distance_3d']:.2f}",
            'std_distance_3d': f"{dist_stats['std_distance_3d']:.2f}",
            'mean_distance_horizontal': f"{dist_stats['mean_distance_horizontal']:.2f}",
            'mean_distance_vertical': f"{dist_stats['mean_distance_vertical']:.2f}"
        }
    
    # GPSç»Ÿè®¡
    if 'gps' in current_analyzer.analysis_results:
        for role, stats in current_analyzer.analysis_results['gps'].items():
            summary['gps'][role] = {
                'data_points': stats['data_points'],
                'total_distance': f"{stats['total_distance']:.2f}",
                'altitude_change': f"{stats['altitude_change']:.2f}",
                'max_speed': f"{stats['max_speed']:.2f}",
                'flight_time': f"{stats['flight_time']:.1f}"
            }
    
    # NEXFIç»Ÿè®¡
    if 'nexfi' in current_analyzer.analysis_results:
        for role, stats in current_analyzer.analysis_results['nexfi'].items():
            summary['nexfi'][role] = {
                'avg_rssi': f"{stats['rssi']['mean']:.2f}",
                'avg_snr': f"{stats['snr']['mean']:.2f}",
                'avg_throughput': f"{stats['throughput']['mean']:.2f}",
                'avg_link_quality': f"{stats['link_quality']['mean']:.2f}"
            }
    
    # ç›¸å…³æ€§åˆ†æ
    if 'correlations' in current_analyzer.analysis_results:
        summary['correlations'] = {}
        for key, corr in current_analyzer.analysis_results['correlations'].items():
            summary['correlations'][key] = {
                'correlation': f"{corr['correlation']:.3f}",
                'p_value': f"{corr['p_value']:.3f}",
                'significant': bool(corr['significant']),
                'data_points': corr['data_points']
            }
    
    # æ—¶é—´èŒƒå›´ä¿¡æ¯
    time_ranges = {}
    
    # UDPæ—¶é—´èŒƒå›´
    if 'udp' in current_analyzer.sender_data and not current_analyzer.sender_data['udp'].empty:
        sender_udp = current_analyzer.sender_data['udp']
        time_ranges['udp_sender'] = {
            'start': sender_udp['timestamp'].min().strftime('%Y-%m-%d %H:%M:%S'),
            'end': sender_udp['timestamp'].max().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    if 'udp' in current_analyzer.receiver_data and not current_analyzer.receiver_data['udp'].empty:
        receiver_udp = current_analyzer.receiver_data['udp']
        time_ranges['udp_receiver'] = {
            'start': receiver_udp['recv_timestamp'].min().strftime('%Y-%m-%d %H:%M:%S'),
            'end': receiver_udp['recv_timestamp'].max().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    # GPSæ—¶é—´èŒƒå›´
    for role in ['sender', 'receiver']:
        if role in current_analyzer.gps_data and not current_analyzer.gps_data[role].empty:
            gps_data = current_analyzer.gps_data[role]
            time_ranges[f'gps_{role}'] = {
                'start': gps_data['timestamp'].min().strftime('%Y-%m-%d %H:%M:%S'),
                'end': gps_data['timestamp'].max().strftime('%Y-%m-%d %H:%M:%S')
            }
    
    # NEXFIæ—¶é—´èŒƒå›´
    for role in ['sender', 'receiver']:
        if role in current_analyzer.nexfi_data and not current_analyzer.nexfi_data[role].empty:
            nexfi_data = current_analyzer.nexfi_data[role]
            time_ranges[f'nexfi_{role}'] = {
                'start': nexfi_data['timestamp'].min().strftime('%Y-%m-%d %H:%M:%S'),
                'end': nexfi_data['timestamp'].max().strftime('%Y-%m-%d %H:%M:%S')
            }
    
    # è®¡ç®—æœ‰æ•ˆæ—¶é—´èŒƒå›´ï¼ˆæ‰€æœ‰æ•°æ®çš„äº¤é›†ï¼‰
    if time_ranges:
        all_starts = []
        all_ends = []
        
        for key, range_info in time_ranges.items():
            all_starts.append(pd.to_datetime(range_info['start']))
            all_ends.append(pd.to_datetime(range_info['end']))
        
        effective_start = max(all_starts)
        effective_end = min(all_ends)
        
        summary['time_range'] = {
            'effective_start': effective_start.strftime('%Y-%m-%d %H:%M:%S'),
            'effective_end': effective_end.strftime('%Y-%m-%d %H:%M:%S'),
            'duration_seconds': (effective_end - effective_start).total_seconds(),
            'individual_ranges': time_ranges
        }
    
    return jsonify(summary)


@app.route('/api/datasets')
def get_datasets():
    """APIç«¯ç‚¹ï¼šè·å–å¯ç”¨æ•°æ®é›†åˆ—è¡¨"""
    scan_available_datasets()
    return jsonify(available_datasets)


@app.route('/api/delete_dataset/<dataset_name>', methods=['DELETE'])
def delete_dataset(dataset_name):
    """APIç«¯ç‚¹ï¼šåˆ é™¤æŒ‡å®šæ•°æ®é›†"""
    global current_analyzer, current_visualizer
    
    try:
        # æŸ¥æ‰¾æ•°æ®é›†è·¯å¾„
        dataset_path = None
        dataset_info = None
        for dataset in available_datasets:
            if dataset['name'] == dataset_name:
                dataset_path = dataset['path']
                dataset_info = dataset
                break
        
        if not dataset_path or not os.path.exists(dataset_path):
            return jsonify({'error': f'æ•°æ®é›† {dataset_name} ä¸å­˜åœ¨'}), 404
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå½“å‰æ­£åœ¨åˆ†æçš„æ•°æ®é›†
        if current_analyzer and hasattr(current_analyzer, 'dataset_path'):
            if os.path.abspath(current_analyzer.dataset_path) == os.path.abspath(dataset_path):
                # æ¸…ç†å½“å‰åˆ†æå™¨å’Œå¯è§†åŒ–å™¨
                current_analyzer = None
                current_visualizer = None
        
        # åˆ é™¤æ•°æ®é›†ç›®å½•
        if os.path.exists(dataset_path):
            shutil.rmtree(dataset_path)
            print(f"å·²åˆ é™¤æ•°æ®é›†: {dataset_path}")
        
        # é‡æ–°æ‰«ææ•°æ®é›†
        scan_available_datasets()
        
        return jsonify({
            'success': True,
            'message': f'æ•°æ®é›† {dataset_name} å·²æˆåŠŸåˆ é™¤',
            'deleted_dataset': dataset_info
        })
        
    except Exception as e:
        print(f"åˆ é™¤æ•°æ®é›†é”™è¯¯: {str(e)}")
        return jsonify({'error': f'åˆ é™¤å¤±è´¥: {str(e)}'}), 500


@app.route('/api/force_reanalyze/<dataset_name>', methods=['POST'])
def force_reanalyze(dataset_name):
    """å¼ºåˆ¶é‡æ–°åˆ†ææ•°æ®é›†"""
    global current_analyzer, current_visualizer
    
    try:
        # æ¸…ç†å½“å‰åˆ†æå™¨å’Œå¯è§†åŒ–å™¨
        current_analyzer = None
        current_visualizer = None
        
        # æŸ¥æ‰¾æ•°æ®é›†è·¯å¾„
        dataset_path = None
        for dataset in available_datasets:
            if dataset['name'] == dataset_name:
                dataset_path = dataset['path']
                break
        
        if not dataset_path or not os.path.exists(dataset_path):
            return jsonify({'error': f'æ•°æ®é›† {dataset_name} ä¸å­˜åœ¨'}), 404
        
        print(f"å¼ºåˆ¶é‡æ–°åˆ†ææ•°æ®é›†: {dataset_name}")
        
        # åˆ›å»ºæ–°çš„åˆ†æå™¨å¹¶è¿è¡Œåˆ†æ
        current_analyzer = DroneCommAnalyzer(dataset_path)
        current_analyzer.run_full_analysis()
        
        # åˆ›å»ºæ–°çš„å¯è§†åŒ–å™¨å¹¶ç”Ÿæˆå›¾è¡¨
        current_visualizer = DroneCommVisualizer(current_analyzer)
        current_visualizer.create_all_plots()
        
        return jsonify({
            'success': True,
            'message': f'æ•°æ®é›† {dataset_name} é‡æ–°åˆ†æå®Œæˆ',
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        print(f"å¼ºåˆ¶é‡æ–°åˆ†æé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'é‡æ–°åˆ†æå¤±è´¥: {str(e)}'}), 500


@app.route('/api/clear_cache', methods=['POST'])
def clear_cache():
    """APIç«¯ç‚¹ï¼šæ¸…ç†æœåŠ¡å™¨ç«¯ç¼“å­˜"""
    global current_analyzer, current_visualizer
    
    try:
        # æ¸…ç†å½“å‰åˆ†æå™¨å’Œå¯è§†åŒ–å™¨
        current_analyzer = None
        current_visualizer = None
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_dirs = ['/tmp', tempfile.gettempdir()]
        for temp_dir in temp_dirs:
            if os.path.exists(temp_dir):
                for item in os.listdir(temp_dir):
                    if item.startswith('tmp') and 'drone' in item.lower():
                        try:
                            item_path = os.path.join(temp_dir, item)
                            if os.path.isdir(item_path):
                                shutil.rmtree(item_path)
                            else:
                                os.remove(item_path)
                        except:
                            pass
        
        # é‡æ–°æ‰«ææ•°æ®é›†
        scan_available_datasets()
        
        return jsonify({
            'success': True,
            'message': 'æœåŠ¡å™¨ç¼“å­˜å·²æ¸…ç†',
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({'error': f'æ¸…ç†ç¼“å­˜å¤±è´¥: {str(e)}'}), 500


@app.route('/api/download_report/<dataset_name>')
def download_report(dataset_name):
    """APIç«¯ç‚¹ï¼šä¸‹è½½åˆ†ææŠ¥å‘Š"""
    if current_analyzer is None or current_visualizer is None:
        return jsonify({'error': 'å°šæœªè¿›è¡Œåˆ†æ'}), 400
    
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        zip_path = os.path.join(temp_dir, f'{dataset_name}_analysis_report.zip')
        
        with zipfile.ZipFile(zip_path, 'w') as zipf:
            # ä¿å­˜åˆ†æç»“æœJSON
            results_json = json.dumps(current_analyzer.analysis_results, indent=2, default=str)
            zipf.writestr(f'{dataset_name}_analysis_results.json', results_json)
            
            # ä¿å­˜æ‘˜è¦ - ç›´æ¥è°ƒç”¨api_summaryçš„é€»è¾‘
            summary_response = api_summary()
            if summary_response.status_code == 200:
                summary_data = summary_response.get_json()
                summary_json = json.dumps(summary_data, indent=2, default=str)
                zipf.writestr(f'{dataset_name}_summary.json', summary_json)
            
            # ä¿å­˜æ‰€æœ‰å›¾è¡¨ä¸ºHTML
            for name, fig in current_visualizer.figures.items():
                html_content = fig.to_html(include_plotlyjs='cdn')
                zipf.writestr(f'plots/{name}.html', html_content)
        
        return send_file(zip_path, as_attachment=True, 
                        download_name=f'{dataset_name}_analysis_report.zip')
    
    except Exception as e:
        return jsonify({'error': f'ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}'}), 500


@app.route('/compare')
def compare_datasets():
    """æ¯”è¾ƒå¤šä¸ªæ•°æ®é›†çš„é¡µé¢"""
    scan_available_datasets()
    return render_template('compare.html', datasets=available_datasets)


@app.route('/api/compare', methods=['POST'])
def compare_multiple_datasets():
    """APIç«¯ç‚¹ï¼šæ¯”è¾ƒå¤šä¸ªæ•°æ®é›†"""
    dataset_names = request.json.get('datasets', [])
    
    if len(dataset_names) < 2:
        return jsonify({'error': 'è‡³å°‘éœ€è¦é€‰æ‹©ä¸¤ä¸ªæ•°æ®é›†è¿›è¡Œæ¯”è¾ƒ'}), 400
    
    comparison_results = {}
    
    for dataset_name in dataset_names:
        # æŸ¥æ‰¾æ•°æ®é›†è·¯å¾„
        dataset_path = None
        for dataset in available_datasets:
            if dataset['name'] == dataset_name:
                dataset_path = dataset['path']
                break
        
        if not dataset_path or not os.path.exists(dataset_path):
            comparison_results[dataset_name] = {'error': 'æ•°æ®é›†ä¸å­˜åœ¨'}
            continue
            
        try:
            analyzer = DroneCommAnalyzer(dataset_path)
            analyzer.run_full_analysis()
            
            # æå–å…³é”®æŒ‡æ ‡
            metrics = {}
            if 'udp' in analyzer.analysis_results:
                metrics['packet_loss_rate'] = analyzer.analysis_results['udp']['packet_loss_rate']
                metrics['avg_delay'] = analyzer.analysis_results['udp']['delay_stats']['mean']
                metrics['max_delay'] = analyzer.analysis_results['udp']['delay_stats']['max']
                metrics['throughput'] = analyzer.analysis_results['udp']['throughput_kbps']
                metrics['total_sent'] = analyzer.analysis_results['udp']['total_sent']
                metrics['total_received'] = analyzer.analysis_results['udp']['total_received']
            
            if 'inter_drone_distance' in analyzer.analysis_results:
                metrics['avg_distance_3d'] = analyzer.analysis_results['inter_drone_distance']['mean_distance_3d']
                metrics['max_distance_3d'] = analyzer.analysis_results['inter_drone_distance']['max_distance_3d']
                metrics['min_distance_3d'] = analyzer.analysis_results['inter_drone_distance']['min_distance_3d']
            
            if 'nexfi' in analyzer.analysis_results:
                for role, stats in analyzer.analysis_results['nexfi'].items():
                    metrics[f'{role}_avg_rssi'] = stats['rssi']['mean']
                    metrics[f'{role}_avg_snr'] = stats['snr']['mean']
                    metrics[f'{role}_avg_link_quality'] = stats['link_quality']['mean']
            
            if 'gps' in analyzer.analysis_results:
                for role, stats in analyzer.analysis_results['gps'].items():
                    metrics[f'{role}_flight_distance'] = stats['total_distance']
                    metrics[f'{role}_flight_time'] = stats['flight_time']
                    metrics[f'{role}_max_speed'] = stats['max_speed']
            
            comparison_results[dataset_name] = metrics
            
        except Exception as e:
            comparison_results[dataset_name] = {'error': str(e)}
    
    return jsonify(comparison_results)


@app.route('/api/trajectory_data')
def get_trajectory_data():
    """APIç«¯ç‚¹ï¼šè·å–è½¨è¿¹å›æ”¾æ•°æ®"""
    if current_analyzer is None:
        return jsonify({'error': 'å°šæœªè¿›è¡Œåˆ†æ'}), 400
    
    try:
        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        print("=== Trajectory Data API Debug ===")
        print(f"current_analyzerå­˜åœ¨: {current_analyzer is not None}")
        
        if current_analyzer:
            print(f"current_analyzerç±»å‹: {type(current_analyzer)}")
            print(f"hasattr gps_data: {hasattr(current_analyzer, 'gps_data')}")
            print(f"hasattr sender_data: {hasattr(current_analyzer, 'sender_data')}")
            print(f"hasattr analysis_results: {hasattr(current_analyzer, 'analysis_results')}")
            
            if hasattr(current_analyzer, 'gps_data'):
                print(f"gps_dataç±»å‹: {type(current_analyzer.gps_data)}")
                print(f"gps_dataæ˜¯å¦ä¸ºNone: {current_analyzer.gps_data is None}")
                if current_analyzer.gps_data:
                    print(f"gps_dataé”®: {list(current_analyzer.gps_data.keys()) if isinstance(current_analyzer.gps_data, dict) else 'Not a dict'}")
                    for role in ['sender', 'receiver']:
                        if role in current_analyzer.gps_data:
                            df = current_analyzer.gps_data[role]
                            print(f"gps_data[{role}]å½¢çŠ¶: {df.shape if hasattr(df, 'shape') else 'No shape'}")
                            if hasattr(df, 'columns'):
                                print(f"gps_data[{role}]åˆ—: {list(df.columns)}")
                            if hasattr(df, 'empty'):
                                print(f"gps_data[{role}]æ˜¯å¦ä¸ºç©º: {df.empty}")
            
            if hasattr(current_analyzer, 'sender_data'):
                print(f"sender_dataç±»å‹: {type(current_analyzer.sender_data)}")
                if isinstance(current_analyzer.sender_data, dict):
                    print(f"sender_dataé”®: {list(current_analyzer.sender_data.keys())}")
                    if 'udp' in current_analyzer.sender_data:
                        udp_df = current_analyzer.sender_data['udp']
                        print(f"sender_data['udp']å½¢çŠ¶: {udp_df.shape if hasattr(udp_df, 'shape') else 'No shape'}")
        
        trajectory_data = {
            'sender': [],
            'receiver': [],
            'timestamps': [],
            'metrics': [],
            'overall_stats': {}
        }
        
        # æ ‡è®°æ˜¯å¦æ‰¾åˆ°äº†çœŸå®æ•°æ®
        has_real_data = False
        
        # å®‰å…¨çš„GPSæ•°æ®å¤„ç†
        try:
            if (hasattr(current_analyzer, 'gps_data') and 
                current_analyzer.gps_data and 
                isinstance(current_analyzer.gps_data, dict)):
                
                print(f"GPSæ•°æ®å­—å…¸é”®: {list(current_analyzer.gps_data.keys())}")
                
                # æ”¶é›†æ‰€æœ‰GPSç‚¹ç”¨äºè®¡ç®—å‚è€ƒç‚¹
                all_lats = []
                all_lons = []
                
                for role in ['sender', 'receiver']:
                    if role in current_analyzer.gps_data:
                        gps_df = current_analyzer.gps_data[role]
                        print(f"GPSæ•°æ®[{role}]: ç±»å‹={type(gps_df)}, å½¢çŠ¶={gps_df.shape if hasattr(gps_df, 'shape') else 'æ— å½¢çŠ¶'}")
                        
                        if (gps_df is not None and 
                            hasattr(gps_df, 'empty') and not gps_df.empty and
                            'latitude' in gps_df.columns and 'longitude' in gps_df.columns):
                            
                            # è¿‡æ»¤æ‰nanå€¼
                            valid_rows = gps_df.dropna(subset=['latitude', 'longitude'])
                            print(f"GPSæ•°æ®[{role}]: æœ‰æ•ˆè¡Œæ•°={len(valid_rows)}/{len(gps_df)}")
                            
                            if len(valid_rows) > 0:
                                all_lats.extend(valid_rows['latitude'].tolist())
                                all_lons.extend(valid_rows['longitude'].tolist())
                
                print(f"æ”¶é›†åˆ°çš„åæ ‡ç‚¹æ•°: {len(all_lats)} ä¸ª")
                
                if all_lats and all_lons:
                    ref_lat = sum(all_lats) / len(all_lats)
                    ref_lon = sum(all_lons) / len(all_lons)
                    
                    print(f"å‚è€ƒç‚¹è®¾å®šä¸º: lat={ref_lat:.6f}, lon={ref_lon:.6f}")
                    
                    # å¤„ç†æ¯ä¸ªè§’è‰²çš„GPSæ•°æ®
                    for role in ['sender', 'receiver']:
                        if role in current_analyzer.gps_data:
                            gps_df = current_analyzer.gps_data[role]
                            
                            if (gps_df is not None and 
                                hasattr(gps_df, 'empty') and not gps_df.empty):
                                
                                # è¿‡æ»¤æ‰nanå€¼
                                valid_rows = gps_df.dropna(subset=['latitude', 'longitude', 'timestamp'])
                                
                                # é™åˆ¶æ•°æ®é‡ä»¥é¿å…è¿‡è½½ï¼Œä½†å–æ›´å¤šæ•°æ®ç‚¹
                                sample_df = valid_rows.head(100) if len(valid_rows) > 100 else valid_rows
                                
                                points_added = 0
                                for index, row in sample_df.iterrows():
                                    try:
                                        lat = float(row['latitude'])
                                        lon = float(row['longitude'])
                                        alt = float(row.get('altitude', 0))
                                        
                                        # ç®€åŒ–çš„åæ ‡è½¬æ¢ï¼ˆè¿‘ä¼¼ï¼‰
                                        x = (lon - ref_lon) * 111320 * math.cos(math.radians(ref_lat))
                                        y = (lat - ref_lat) * 111320
                                        
                                        trajectory_data[role].append({
                                            'x': round(x, 2),
                                            'y': round(y, 2),
                                            'z': round(alt, 2),
                                            'timestamp': row['timestamp'].isoformat()
                                        })
                                        
                                        points_added += 1
                                        has_real_data = True
                                        
                                    except Exception as e:
                                        print(f"å¤„ç†{role} GPSæ•°æ®è¡Œé”™è¯¯: {e}")
                                        continue
                                
                                print(f"ä¸º{role}æ·»åŠ äº†{points_added}ä¸ªGPSç‚¹")
                else:
                    print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„GPSåæ ‡æ•°æ®")
                    
        except Exception as e:
            print(f"GPSæ•°æ®å¤„ç†é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        
        # å®‰å…¨çš„UDPæŒ‡æ ‡æ•°æ®å¤„ç†
        try:
            metrics_added = 0
            
            # ä¼˜å…ˆä½¿ç”¨æ¥æ”¶æ–¹æ•°æ®ï¼Œå› ä¸ºå®ƒåŒ…å«å»¶è¿Ÿä¿¡æ¯
            if (hasattr(current_analyzer, 'receiver_data') and 
                isinstance(current_analyzer.receiver_data, dict) and
                'udp' in current_analyzer.receiver_data and 
                current_analyzer.receiver_data['udp'] is not None and
                not current_analyzer.receiver_data['udp'].empty):
                
                udp_df = current_analyzer.receiver_data['udp']
                print(f"ä½¿ç”¨æ¥æ”¶æ–¹UDPæ•°æ®ï¼Œå½¢çŠ¶: {udp_df.shape}")
                print(f"æ¥æ”¶æ–¹UDPåˆ—: {list(udp_df.columns)}")
                
                # é™åˆ¶æ•°æ®é‡ï¼Œå–å‰50ä¸ªæ•°æ®ç‚¹
                sample_df = udp_df.head(50) if len(udp_df) > 50 else udp_df
                
                for index, row in sample_df.iterrows():
                    try:
                        if pd.notna(row.get('recv_timestamp')):
                            delay_value = None
                            if pd.notna(row.get('delay')):
                                delay_value = float(row['delay']) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                            
                            trajectory_data['metrics'].append({
                                'timestamp': row['recv_timestamp'].isoformat(),
                                'delay': delay_value,
                                'packet_loss': 0
                            })
                            metrics_added += 1
                            has_real_data = True
                    except Exception as e:
                        print(f"å¤„ç†æ¥æ”¶æ–¹UDPæ•°æ®è¡Œé”™è¯¯: {e}")
                        continue
                
                print(f"ä»æ¥æ”¶æ–¹æ·»åŠ äº†{metrics_added}ä¸ªUDPæŒ‡æ ‡æ•°æ®ç‚¹")
                
            # å¦‚æœæ¥æ”¶æ–¹æ•°æ®ä¸å¯ç”¨ï¼Œå°è¯•å‘é€æ–¹æ•°æ®
            elif (hasattr(current_analyzer, 'sender_data') and 
                  isinstance(current_analyzer.sender_data, dict) and
                  'udp' in current_analyzer.sender_data and 
                  current_analyzer.sender_data['udp'] is not None and
                  not current_analyzer.sender_data['udp'].empty):
                
                udp_df = current_analyzer.sender_data['udp']
                print(f"ä½¿ç”¨å‘é€æ–¹UDPæ•°æ®ï¼Œå½¢çŠ¶: {udp_df.shape}")
                
                # é™åˆ¶æ•°æ®é‡ï¼Œå–å‰50ä¸ªæ•°æ®ç‚¹
                sample_df = udp_df.head(50) if len(udp_df) > 50 else udp_df
                
                for index, row in sample_df.iterrows():
                    try:
                        if pd.notna(row.get('timestamp')):
                            trajectory_data['metrics'].append({
                                'timestamp': row['timestamp'].isoformat(),
                                'delay': None,  # å‘é€æ–¹æ•°æ®é€šå¸¸æ²¡æœ‰å»¶è¿Ÿä¿¡æ¯
                                'packet_loss': 0
                            })
                            metrics_added += 1
                            has_real_data = True
                    except Exception as e:
                        print(f"å¤„ç†å‘é€æ–¹UDPæ•°æ®è¡Œé”™è¯¯: {e}")
                        continue
                
                print(f"ä»å‘é€æ–¹æ·»åŠ äº†{metrics_added}ä¸ªUDPæŒ‡æ ‡æ•°æ®ç‚¹")
                        
        except Exception as e:
            print(f"UDPæ•°æ®å¤„ç†é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        
        # å®‰å…¨çš„ç»Ÿè®¡ä¿¡æ¯å¤„ç†
        try:
            if (hasattr(current_analyzer, 'analysis_results') and 
                current_analyzer.analysis_results and 
                'udp' in current_analyzer.analysis_results):
                
                udp_stats = current_analyzer.analysis_results['udp']
                trajectory_data['overall_stats'] = {
                    'packet_loss_rate': udp_stats.get('packet_loss_rate', 0),
                    'avg_delay': udp_stats.get('delay_stats', {}).get('mean', 0),
                    'max_delay': udp_stats.get('delay_stats', {}).get('max', 0)
                }
                print(f"æ·»åŠ äº†UDPç»Ÿè®¡ä¿¡æ¯: {trajectory_data['overall_stats']}")
        except Exception as e:
            print(f"ç»Ÿè®¡ä¿¡æ¯å¤„ç†é”™è¯¯: {e}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸå®æ•°æ®ï¼Œè¿”å›ç¤ºä¾‹æ•°æ®ï¼Œä½†æ ‡è®°ä¸ºç¤ºä¾‹
        if not has_real_data:
            print("æ²¡æœ‰æ‰¾åˆ°çœŸå®æ•°æ®ï¼Œè¿”å›ç¤ºä¾‹æ•°æ®")
            trajectory_data = {
                'sender': [
                    {'x': 0, 'y': 0, 'z': 0, 'timestamp': '2025-01-01T00:00:00'},
                    {'x': 10, 'y': 10, 'z': 5, 'timestamp': '2025-01-01T00:01:00'}
                ],
                'receiver': [
                    {'x': 100, 'y': 0, 'z': 0, 'timestamp': '2025-01-01T00:00:00'},
                    {'x': 90, 'y': 10, 'z': 5, 'timestamp': '2025-01-01T00:01:00'}
                ],
                'timestamps': ['2025-01-01T00:00:00', '2025-01-01T00:01:00'],
                'metrics': [
                    {'timestamp': '2025-01-01T00:00:00', 'delay': 50, 'packet_loss': 0},
                    {'timestamp': '2025-01-01T00:01:00', 'delay': 75, 'packet_loss': 0}
                ],
                'overall_stats': {
                    'packet_loss_rate': 0,
                    'avg_delay': 62.5,
                    'max_delay': 75
                },
                'is_sample_data': True
            }
        else:
            trajectory_data['is_sample_data'] = False
            print(f"è¿”å›çœŸå®æ•°æ®: sender={len(trajectory_data['sender'])}ç‚¹, receiver={len(trajectory_data['receiver'])}ç‚¹, metrics={len(trajectory_data['metrics'])}ç‚¹")
        
        return jsonify(trajectory_data)
        
    except Exception as e:
        print(f"è½¨è¿¹æ•°æ®APIæ€»ä½“é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        # è¿”å›åŸºæœ¬å“åº”è€Œä¸æ˜¯é”™è¯¯
        return jsonify({
            'sender': [],
            'receiver': [],
            'timestamps': [],
            'metrics': [],
            'overall_stats': {},
            'error': f'æ•°æ®å¤„ç†é”™è¯¯: {str(e)}',
            'is_sample_data': False
        })


if __name__ == '__main__':
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)
    os.makedirs('data', exist_ok=True)
    os.makedirs('uploads', exist_ok=True)
    
    # è·å–ç«¯å£é…ç½®ï¼ˆæ”¯æŒRenderéƒ¨ç½²ï¼‰
    port = int(os.environ.get('PORT', 6500))
    host = '0.0.0.0'
    
    print("ğŸš æ— äººæœºé€šä¿¡æ•°æ®åˆ†æç³»ç»Ÿæ­£åœ¨å¯åŠ¨...")
    print(f"ğŸ“Š ç³»ç»Ÿå°†åœ¨ http://{host}:{port} å¯åŠ¨")
    print("ğŸŒ è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®ä¸Šè¿°åœ°å€")
    
    # æ‰«æå¯ç”¨æ•°æ®é›†
    scan_available_datasets()
    print(f"ğŸ“‚ å‘ç° {len(available_datasets)} ä¸ªæ•°æ®é›†")
    
    # æ ¹æ®ç¯å¢ƒåˆ¤æ–­æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
    debug_mode = os.environ.get('FLASK_ENV') != 'production'
    
    app.run(debug=debug_mode, host=host, port=port) 